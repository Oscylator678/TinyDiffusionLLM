{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae626c7-be33-408d-a1a0-e300a63a4aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdb7f9e-89bc-4db3-ae68-1ab52033ef3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data\n",
    "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip\n",
    "!unzip wikitext-103-v1.zip\n",
    "!mv wikitext-103/wiki.train.tokens data/train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f748db50-015f-458f-b40c-9583b401b90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 1. CONFIGURATION & TOKENIZER\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "SPECIAL_TOKEN_MAP = {\n",
    "    \"<PAD>\":    0xC0,  # 192\n",
    "    \"<EOS>\":    0xC1,  # 193\n",
    "    \"<DELETE>\": 0xF5,  # 245\n",
    "    \"<EXPAND>\": 0xF6,  # 246\n",
    "    \"<MASK>\":   0xF7,  # 247\n",
    "}\n",
    "ID_TO_SPECIAL = {v: k for k, v in SPECIAL_TOKEN_MAP.items()}\n",
    "VOCAB_SIZE = 256\n",
    "\n",
    "class ByteTokenizer:\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        return list(text.encode(\"utf-8\"))\n",
    "\n",
    "    def decode(self, tokens: list[int]) -> str:\n",
    "        valid_bytes = bytearray()\n",
    "        for t in tokens:\n",
    "            if t not in ID_TO_SPECIAL:\n",
    "                valid_bytes.append(t)\n",
    "        return valid_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "def process_large_file(model, text, device, window_size=512, overlap=256):\n",
    "    \"\"\"\n",
    "    Processes a long string by sliding a window with overlap.\n",
    "    Merges outputs using a linear fade (sigmoid-like) weight to prevent boundary seams.\n",
    "    \"\"\"\n",
    "    tokenizer = ByteTokenizer()\n",
    "    data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "    N = len(data)\n",
    "    \n",
    "    # We will accumulate weighted logits for each position\n",
    "    # Shape: (N, Vocab_Size)\n",
    "    # Note: For huge files, keep this on CPU to save VRAM/RAM\n",
    "    all_logits = torch.zeros(N, 256, device=\"cpu\")\n",
    "    weights = torch.zeros(N, device=\"cpu\")\n",
    "    \n",
    "    # Create a \"fade mask\" (trapezoid shape) for the window\n",
    "    # 0 -> 1 -> 1 -> 0\n",
    "    # This ensures we trust the center of the window most\n",
    "    ramp = torch.linspace(0, 1, overlap)\n",
    "    window_mask = torch.ones(window_size)\n",
    "    window_mask[:overlap] = ramp\n",
    "    window_mask[-overlap:] = 1 - ramp\n",
    "    \n",
    "    model.eval()\n",
    "    step = window_size - overlap\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for start in range(0, N, step):\n",
    "            end = min(start + window_size, N)\n",
    "            \n",
    "            # Prepare chunk\n",
    "            chunk = data[start:end]\n",
    "            pad_len = window_size - len(chunk)\n",
    "            \n",
    "            # If chunk is too short (end of file), pad it\n",
    "            if pad_len > 0:\n",
    "                chunk = torch.cat([chunk, torch.full((pad_len,), 0xC0, dtype=torch.long)])\n",
    "            \n",
    "            # Run Model\n",
    "            inp = chunk.unsqueeze(0).to(device) # (1, 512)\n",
    "            \n",
    "            # Get logits\n",
    "            # For diffusion, you might run this repeatedly. \n",
    "            # Here we assume a single pass corrector for simplicity.\n",
    "            logits = model(inp)[0].cpu() # (512, 256)\n",
    "            \n",
    "            # Remove padding from output\n",
    "            valid_len = end - start\n",
    "            logits = logits[:valid_len]\n",
    "            mask = window_mask[:valid_len]\n",
    "            \n",
    "            # Accumulate\n",
    "            # We add the weighted logits to the global buffer\n",
    "            # (broadcasting mask to shape (Len, 1))\n",
    "            all_logits[start:end] += logits * mask.unsqueeze(1)\n",
    "            weights[start:end] += mask\n",
    "            \n",
    "    # Normalize and Decode\n",
    "    # Avoid division by zero\n",
    "    weights[weights == 0] = 1.0 \n",
    "    final_logits = all_logits / weights.unsqueeze(1)\n",
    "    \n",
    "    pred_ids = final_logits.argmax(dim=-1).tolist()\n",
    "    return tokenizer.decode(pred_ids)\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. MEMORY-SAFE DATA PIPELINE (Streaming)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class VocabAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.counts = np.zeros(256, dtype=np.int64)\n",
    "        self.total = 0\n",
    "        self.counts += 1 \n",
    "\n",
    "    def update(self, text_bytes):\n",
    "        # FAST: Read bytes directly into numpy array (no Python list overhead)\n",
    "        # np.frombuffer is zero-copy for bytes\n",
    "        arr = np.frombuffer(text_bytes, dtype=np.uint8)\n",
    "        unique, counts = np.unique(arr, return_counts=True)\n",
    "        \n",
    "        # Fast accumulation\n",
    "        for u, c in zip(unique, counts):\n",
    "            # 256 is the limit for byte values\n",
    "            if u < 256 and u not in ID_TO_SPECIAL:\n",
    "                self.counts[u] += c\n",
    "        self.total += len(text_bytes)\n",
    "\n",
    "    def get_probs(self):\n",
    "        probs = self.counts.astype(np.float32)\n",
    "        for sp_id in SPECIAL_TOKEN_MAP.values():\n",
    "            if sp_id < 256: probs[sp_id] = 0.0\n",
    "        return probs / probs.sum()\n",
    "\n",
    "class CorruptionEngine:\n",
    "    def __init__(self, vocab_probs=None):\n",
    "        self.vocab_probs = vocab_probs if vocab_probs is not None else np.ones(256)/256\n",
    "\n",
    "    def get_noise_token(self):\n",
    "        return np.random.choice(256, p=self.vocab_probs)\n",
    "\n",
    "    def process_segment(self, clean_bytes):\n",
    "        src = []\n",
    "        tgt = []\n",
    "        i = 0\n",
    "        n = len(clean_bytes)\n",
    "        \n",
    "        while i < n:\n",
    "            r = random.random()\n",
    "            # Op 1: Insertion\n",
    "            if r < 0.05: \n",
    "                src.append(self.get_noise_token())\n",
    "                tgt.append(SPECIAL_TOKEN_MAP[\"<DELETE>\"])\n",
    "                continue\n",
    "            # Op 2: Expansion\n",
    "            elif r < 0.10:\n",
    "                span_len = random.randint(1, 8)\n",
    "                span_len = min(span_len, n - i)\n",
    "                src.append(SPECIAL_TOKEN_MAP[\"<EXPAND>\"])\n",
    "                src.extend([SPECIAL_TOKEN_MAP[\"<PAD>\"]] * (span_len - 1))\n",
    "                tgt.extend([SPECIAL_TOKEN_MAP[\"<MASK>\"]] * span_len)\n",
    "                i += span_len\n",
    "                continue\n",
    "            # Op 3: Masking\n",
    "            elif r < 0.25:\n",
    "                src.append(SPECIAL_TOKEN_MAP[\"<MASK>\"])\n",
    "                tgt.append(clean_bytes[i])\n",
    "                i += 1\n",
    "            # Identity\n",
    "            else:\n",
    "                src.append(clean_bytes[i])\n",
    "                tgt.append(clean_bytes[i])\n",
    "                i += 1\n",
    "        return src, tgt\n",
    "\n",
    "def correction_data_generator(filename_pattern, batch_size, max_seq_len, device=\"cpu\"):\n",
    "    files = sorted(glob.glob(filename_pattern))\n",
    "    if not files:\n",
    "        print(\"No files found.\")\n",
    "        files = []\n",
    "\n",
    "    # 1. Quick Histogram (Read only 1st MB)\n",
    "    print(\"Building vocabulary histogram...\")\n",
    "    analyzer = VocabAnalyzer()\n",
    "    sample_bytes = 0\n",
    "    for fpath in files:\n",
    "        if sample_bytes > 1024*1024: break\n",
    "        try:\n",
    "            with open(fpath, 'rb') as f:\n",
    "                analyzer.update(f.read(50000))\n",
    "                sample_bytes += 50000\n",
    "        except: continue\n",
    "    vocab_probs = analyzer.get_probs()\n",
    "\n",
    "    tokenizer = ByteTokenizer()\n",
    "    engine = CorruptionEngine(vocab_probs)\n",
    "    file_cycle = itertools.cycle(files) if files else None\n",
    "    \n",
    "    # 2. Streaming Loop\n",
    "    # We maintain a buffer to assemble batches across file chunks\n",
    "    buffer_src = []\n",
    "    buffer_tgt = []\n",
    "    \n",
    "    # Read files in small chunks (e.g. 1MB) to prevent RAM explosion\n",
    "    CHUNK_SIZE = 1024 * 1024 \n",
    "    \n",
    "    while True:\n",
    "        if not files:\n",
    "            # Dummy data fallback\n",
    "            clean_bytes = tokenizer.encode(\"Hello world, this is a test. \" * 50)\n",
    "            s, t = engine.process_segment(clean_bytes)\n",
    "            buffer_src.extend(s)\n",
    "            buffer_tgt.extend(t)\n",
    "        else:\n",
    "            curr_file = next(file_cycle)\n",
    "            try:\n",
    "                with open(curr_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    while True:\n",
    "                        # READ CHUNK INSTEAD OF WHOLE FILE\n",
    "                        text_chunk = f.read(CHUNK_SIZE)\n",
    "                        if not text_chunk: break\n",
    "                        \n",
    "                        clean_bytes = tokenizer.encode(text_chunk)\n",
    "                        s, t = engine.process_segment(clean_bytes)\n",
    "                        buffer_src.extend(s)\n",
    "                        buffer_tgt.extend(t)\n",
    "                        \n",
    "                        # Yield batches as soon as we have enough data\n",
    "                        while len(buffer_src) >= batch_size * max_seq_len:\n",
    "                            # Extract one batch worth of tokens\n",
    "                            # Note: We cut strictly by length here. \n",
    "                            # Ideally we'd respect sentences, but for char-level diffusion \n",
    "                            # strict chunking is acceptable training noise.\n",
    "                            batch_src_list = []\n",
    "                            batch_tgt_list = []\n",
    "                            \n",
    "                            # Slice out 'batch_size' sequences\n",
    "                            total_needed = batch_size * max_seq_len\n",
    "                            \n",
    "                            # Grab raw tokens\n",
    "                            raw_s = buffer_src[:total_needed]\n",
    "                            raw_t = buffer_tgt[:total_needed]\n",
    "                            \n",
    "                            # Clear from buffer\n",
    "                            buffer_src = buffer_src[total_needed:]\n",
    "                            buffer_tgt = buffer_tgt[total_needed:]\n",
    "                            \n",
    "                            # Reshape into (B, T)\n",
    "                            for i in range(0, total_needed, max_seq_len):\n",
    "                                batch_src_list.append(raw_s[i : i+max_seq_len])\n",
    "                                batch_tgt_list.append(raw_t[i : i+max_seq_len])\n",
    "                                \n",
    "                            _inputs = torch.tensor(batch_src_list, dtype=torch.long)\n",
    "                            _targets = torch.tensor(batch_tgt_list, dtype=torch.long)\n",
    "                            \n",
    "                            use_non_blocking = (device == \"cuda\")\n",
    "                            yield (\n",
    "                                _inputs.to(device=device, non_blocking=use_non_blocking),\n",
    "                                _targets.to(device=device, non_blocking=use_non_blocking)\n",
    "                            )\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {curr_file}: {e}\")\n",
    "                continue\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. OPTIMIZER & MATH\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "coeffs_list = [\n",
    "    (8.2872, -23.5959, 17.3004), (4.1071, -2.9478, 0.5448), \n",
    "    (3.9487, -2.9089, 0.5518), (3.3184, -2.4885, 0.5100), \n",
    "    (2.3007, -1.6689, 0.4188), (1.8913, -1.2680, 0.3768), \n",
    "    (1.8750, -1.2500, 0.3750), (1.875, -1.25, 0.375)\n",
    "]\n",
    "coeffs_list = [(a/1.01, b/1.01**3, c/1.01**5) for (a,b,c) in coeffs_list[:-1]] + [coeffs_list[-1]]\n",
    "\n",
    "def polar_express(G: torch.Tensor, steps: int) -> torch.Tensor:\n",
    "    assert G.ndim == 2\n",
    "    dtype = torch.bfloat16 if (G.device.type == 'cpu' and torch.cuda.is_bf16_supported()) else torch.float32\n",
    "    X = G.to(dtype=dtype)\n",
    "    if G.size(0) > G.size(1): X = X.T\n",
    "    X = X / (X.norm() + 1e-7)\n",
    "    hs = coeffs_list[:steps] + list(itertools.repeat(coeffs_list[-1], steps - len(coeffs_list)))\n",
    "    for a, b, c in hs:\n",
    "        A = X @ X.T\n",
    "        B = b * A + c * (A @ A)\n",
    "        X = a * X + B @ X\n",
    "    if G.size(0) > G.size(1): X = X.T\n",
    "    return X.to(dtype=G.dtype)\n",
    "\n",
    "class NorMuon(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=0.02, momentum=0.9, weight_decay=0.01, \n",
    "                 ns_steps=5, adam_lr=1e-3, adam_betas=(0.9, 0.95), epsilon=1e-8):\n",
    "        defaults = dict(lr=lr, momentum=momentum, weight_decay=weight_decay, \n",
    "                        ns_steps=ns_steps, epsilon=epsilon,\n",
    "                        adam_lr=adam_lr, adam_betas=adam_betas)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure: loss = closure()\n",
    "        for group in self.param_groups:\n",
    "            lr, beta_m, wd = group['lr'], group['momentum'], group['weight_decay']\n",
    "            adam_lr, (beta1, beta2) = group['adam_lr'], group['adam_betas']\n",
    "            for p in group['params']:\n",
    "                if p.grad is None: continue\n",
    "                grad = p.grad\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['momentum'] = torch.zeros_like(p)\n",
    "                    state['exp_avg'] = torch.zeros_like(p)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p)\n",
    "                    if p.ndim >= 2:\n",
    "                        state['vt'] = torch.zeros(p.size(0), 1, device=p.device, dtype=p.dtype)\n",
    "                state['step'] += 1\n",
    "                if p.ndim >= 2:\n",
    "                    p_flat = p.view(p.size(0), -1)\n",
    "                    g_flat = grad.view(p.size(0), -1)\n",
    "                    buf = state['momentum'].view(p.size(0), -1)\n",
    "                    buf.mul_(beta_m).add_(g_flat, alpha=1 - beta_m)\n",
    "                    Ot = polar_express(buf, steps=group['ns_steps'])\n",
    "                    Ot_sq_mean = Ot.square().mean(dim=1, keepdim=True)\n",
    "                    vt = state['vt']\n",
    "                    vt.mul_(beta1).add_(Ot_sq_mean, alpha=1 - beta1)\n",
    "                    O_hat = Ot / (vt + group['epsilon'])\n",
    "                    scale = 0.2 * lr * (p_flat.shape[0]*p_flat.shape[1])**0.5 / (O_hat.norm() + 1e-9)\n",
    "                    p_flat.mul_(1 - lr * wd)\n",
    "                    p_flat.add_(O_hat, alpha=-scale)\n",
    "                else:\n",
    "                    exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                    p.mul_(1 - adam_lr * wd)\n",
    "                    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "                    exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['epsilon'])\n",
    "                    p.addcdiv_(exp_avg, denom, value=-adam_lr)\n",
    "        return loss\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. MODEL (Optimized for ~10M Params)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_seq_len=1024):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        t = torch.arange(max_seq_len).float()\n",
    "        freqs = torch.einsum('i,j->ij', t, inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        # Cache as (1, 1, T, D) for broadcasting against (B, H, T, D)\n",
    "        self.register_buffer('cos_cached', emb.cos()[None, None, :, :])\n",
    "        self.register_buffer('sin_cached', emb.sin()[None, None, :, :])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, H, T, D)\n",
    "        seq_len = x.shape[2]\n",
    "        return self.cos_cached[:, :, :seq_len, :], self.sin_cached[:, :, :seq_len, :]\n",
    "\n",
    "def apply_rope(q, k, cos, sin):\n",
    "    def rotate_half(x):\n",
    "        x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]\n",
    "        return torch.cat((-x2, x1), dim=-1)\n",
    "    q_out = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_out = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_out, k_out\n",
    "\n",
    "class BidirectionalAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # 1. SETUP DIMENSIONS\n",
    "        # Standard: head_dim = 48 (for 384 model / 8 heads)\n",
    "        # Paired Head Strategy: Squeeze QK (speed), Expand V (capacity)\n",
    "        \n",
    "        self.n_head = config.n_head\n",
    "        self.embd = config.n_embd\n",
    "        \n",
    "        \n",
    "        self.qk_head_dim = config.qk_head_dim \n",
    "        \n",
    "        self.v_head_dim = config.v_head_dim\n",
    "        \n",
    "        # 2. DECOUPLED PROJECTIONS\n",
    "        # We can no longer use a single Linear(dim, 3*dim)\n",
    "        \n",
    "        # Query & Key: Output size = n_head * 32\n",
    "        self.q_proj = nn.Linear(self.embd, self.n_head * self.qk_head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(self.embd, self.n_head * self.qk_head_dim, bias=False)\n",
    "        \n",
    "        # Value: Output size = n_head * 64\n",
    "        self.v_proj = nn.Linear(self.embd, self.n_head * self.v_head_dim, bias=False)\n",
    "        \n",
    "        # Output Projection: Maps n_head * 64 back to model dim (384)\n",
    "        self.c_proj = nn.Linear(self.n_head * self.v_head_dim, self.embd, bias=False)\n",
    "\n",
    "    def forward(self, x, rope_cos, rope_sin):\n",
    "        B, T, C = x.size()\n",
    "        \n",
    "        # 1. Project Q, K (Small)\n",
    "        q = self.q_proj(x).view(B, T, self.n_head, self.qk_head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(x).view(B, T, self.n_head, self.qk_head_dim).transpose(1, 2)\n",
    "        \n",
    "        # 2. Project V (Large)\n",
    "        v = self.v_proj(x).view(B, T, self.n_head, self.v_head_dim).transpose(1, 2)\n",
    "        \n",
    "        # 3. Apply RoPE (Only to Q and K)\n",
    "        # Note: RoPE implementation needs to accept the smaller qk_head_dim now\n",
    "        # You must ensure the RoPE cache logic handles 'qk_head_dim' (32), not 'v_head_dim' (64).\n",
    "        # Since our fixed RoPE creates freq based on input size, it should work if slice is correct.\n",
    "        q, k = apply_rope(q, k, rope_cos, rope_sin)\n",
    "        \n",
    "        # 4. Attention\n",
    "        # PyTorch F.sdpa supports different V dimensions natively!\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=False)\n",
    "        \n",
    "        # 5. Reassemble\n",
    "        # y shape: (B, Heads, T, v_head_dim)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, self.n_head * self.v_head_dim)\n",
    "        \n",
    "        return self.c_proj(y)\n",
    "\n",
    "# class BidirectionalAttention(nn.Module):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__()\n",
    "#         assert config.n_embd % config.n_head == 0\n",
    "#         self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=False)\n",
    "#         self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
    "#         self.n_head = config.n_head\n",
    "#         self.head_dim = config.n_embd // config.n_head\n",
    "\n",
    "#     def forward(self, x, rope_cos, rope_sin):\n",
    "#         B, T, C = x.size()\n",
    "#         qkv = self.c_attn(x).chunk(3, dim=2)\n",
    "#         q, k, v = [t.view(B, T, self.n_head, self.head_dim).transpose(1, 2) for t in qkv]\n",
    "#         q, k = apply_rope(q, k, rope_cos, rope_sin)\n",
    "#         y = F.scaled_dot_product_attention(q, k, v, is_causal=False)\n",
    "#         y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "#         return self.c_proj(y)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.c_proj(self.gelu(self.c_fc(x)))\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = BidirectionalAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x, rope_cos, rope_sin):\n",
    "        x = x + self.attn(self.ln_1(x), rope_cos, rope_sin)\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 4096\n",
    "    vocab_size: int = 256\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 8\n",
    "    n_embd: int = 256\n",
    "    qk_head_dim: int = 32 # QK Head Dim: Keep small (e.g., 32) for fast attention score calc\n",
    "    v_head_dim: int = 64 # V Head Dim: Keep large (e.g., 64) for moving more information\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.token_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        \n",
    "        # REMOVED: BigramEmbedding (Saved ~25M params)\n",
    "        # To restore: Uncomment and add \" + self.bigram_emb(idx)\" in forward\n",
    "        # self.bigram_emb = nn.Embedding(config.vocab_size**2, config.n_embd)\n",
    "\n",
    "        # self.rope = RotaryEmbedding(config.n_embd // config.n_head, config.block_size)\n",
    "        # To matching the qk_head_dim=32 we set above):\n",
    "        self.rope = RotaryEmbedding(config.qk_head_dim, config.block_size)\n",
    "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "        self.token_emb.weight = self.lm_head.weight\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None: torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        \n",
    "        # Standard embedding\n",
    "        x = self.token_emb(idx) \n",
    "        \n",
    "        # Dummy RoPE call to get cos/sin for current seq_len\n",
    "        # (Pass shape B,H,T,D -> we just need T)\n",
    "        dummy_x = x.unsqueeze(1) \n",
    "        cos, sin = self.rope(dummy_x)\n",
    "        \n",
    "        skips = []\n",
    "        half_layers = len(self.blocks) // 2\n",
    "        \n",
    "        for i, block in enumerate(self.blocks):\n",
    "            if i < half_layers:\n",
    "                x = block(x, cos, sin)\n",
    "                skips.append(x)\n",
    "            else:\n",
    "                skip_val = skips.pop()\n",
    "                x = x + skip_val \n",
    "                x = block(x, cos, sin)\n",
    "                \n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. MAIN (With Resume & Safe Paths)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Configuration ---\n",
    "    SCRIPT_DIR = os.getcwd() \n",
    "    DATA_DIR = os.path.join(SCRIPT_DIR, \"data\")\n",
    "    \n",
    "    # SAVE CHECKPOINTS TO DRIVE INSTEAD\n",
    "    # Create a folder in your Drive named 'diffusion_checkpoints'\n",
    "    CKPT_DIR = \"/content/drive/MyDrive/diffusion_checkpoints\" \n",
    "    os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "    TRAIN_PATTERN = os.path.join(DATA_DIR, \"*.txt\")\n",
    "    BATCH_SIZE = 256\n",
    "    SEQ_LEN = 1024\n",
    "    MAX_STEPS = 10**7\n",
    "    SAVE_EVERY = 10**3\n",
    "    \n",
    "    DEVICE = \"cpu\"\n",
    "    if torch.cuda.is_available(): DEVICE = \"cuda\"\n",
    "    elif torch.backends.mps.is_available(): DEVICE = \"mps\"\n",
    "    \n",
    "    print(f\"--- Training Config ---\")\n",
    "    print(f\"Device:      {DEVICE}\")\n",
    "    print(f\"Data Path:   {TRAIN_PATTERN}\")\n",
    "    print(f\"Ckpt Path:   {CKPT_DIR}\")\n",
    "    print(f\"Arch:        ~10M Params, Bidirectional, RoPE, U-Net, Pair head attention\")\n",
    "    \n",
    "    # --- Init Model & Optimizer ---\n",
    "    config = GPTConfig(block_size=SEQ_LEN)\n",
    "    model = GPT(config).to(DEVICE)\n",
    "    \n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Model parameters: {n_params/1e6:.2f}M\")\n",
    "\n",
    "    optimizer = NorMuon(model.parameters(), lr=0.05, adam_lr=0.001, ns_steps=5)\n",
    "    \n",
    "    # --- Checkpoint Loading Logic ---\n",
    "    start_step = 0\n",
    "    # Find all checkpoint_*.pt files\n",
    "    ckpt_files = glob.glob(os.path.join(CKPT_DIR, \"checkpoint_*.pt\"))\n",
    "    if ckpt_files:\n",
    "        try:\n",
    "            # Find latest checkpoint\n",
    "            latest_ckpt = max(ckpt_files, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "            print(f\"Attempting to resume from: {latest_ckpt}\")\n",
    "\n",
    "            # 1. OPTION A: Secure Load (PyTorch 2.6+)\n",
    "            # We must tell PyTorch that GPTConfig is safe to unpickle\n",
    "            try:\n",
    "                import torch.serialization\n",
    "                torch.serialization.add_safe_globals([GPTConfig])\n",
    "                checkpoint = torch.load(latest_ckpt, map_location=DEVICE, weights_only=True)\n",
    "                state_dict = checkpoint['model_state']\n",
    "                \n",
    "                # FIX: Remove position embeddings from the loaded state\n",
    "                # The new model has already generated correct ones for the new SEQ_LEN\n",
    "                keys_to_remove = [\"rope.cos_cached\", \"rope.sin_cached\"]\n",
    "                for k in keys_to_remove:\n",
    "                    if k in state_dict:\n",
    "                        del state_dict[k]\n",
    "                \n",
    "                # Use strict=False to allow missing RoPE buffers\n",
    "                model.load_state_dict(state_dict, strict=False)\n",
    "            except (AttributeError, RuntimeError, ImportError):\n",
    "                # Fallback for older PyTorch or if whitelist fails\n",
    "                print(\"Warning: Secure load failed, falling back to weights_only=False\")\n",
    "                checkpoint = torch.load(latest_ckpt, map_location=DEVICE, weights_only=False)\n",
    "\n",
    "            # 2. Load Model Weights (Strict=True ensures architecture matches)\n",
    "            model.load_state_dict(checkpoint['model_state'])\n",
    "            print(\"Model weights loaded.\")\n",
    "\n",
    "            # 3. Clever Optimizer Load\n",
    "            # We try to load the optimizer, but if it fails (e.g. slight mismatch), \n",
    "            # we skip it and restart the optimizer logic.\n",
    "            try:\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "                start_step = checkpoint['step'] + 1\n",
    "                print(f\"Optimizer state loaded. Resuming at step {start_step}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load optimizer state ({e}).\")\n",
    "                print(\"Reseting optimizer and continuing training from current weights.\")\n",
    "                # start_step remains 0 (or you can set it to checkpoint['step'] if you just want to track progress)\n",
    "                # Usually if we reset optimizer, we treat it as a 'finetune' start, but keeping step count is often useful for logs.\n",
    "                start_step = checkpoint['step'] + 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"CRITICAL: Failed to load checkpoint {latest_ckpt}: {e}\")\n",
    "            print(\"Starting training from scratch.\")\n",
    "\n",
    "    # --- Data Loader ---\n",
    "    train_loader = correction_data_generator(TRAIN_PATTERN, BATCH_SIZE, SEQ_LEN, DEVICE)\n",
    "    \n",
    "    # --- Training Loop ---\n",
    "    model.train()\n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    # Adjust range to start from resumed step\n",
    "    for step in range(start_step, MAX_STEPS):\n",
    "        inputs, targets = next(train_loader)\n",
    "        \n",
    "        logits = model(inputs)\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(-1, VOCAB_SIZE), \n",
    "            targets.view(-1), \n",
    "            ignore_index=SPECIAL_TOKEN_MAP[\"<PAD>\"]\n",
    "        )\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            t1 = time.time()\n",
    "            dt = (t1 - t0) * 1000\n",
    "            t0 = t1 # Reset timer\n",
    "            print(f\"Step {step:4d} | Loss: {loss.item():.4f} | Time: {dt:.2f}ms\")\n",
    "            \n",
    "        if step % SAVE_EVERY == 0 and step > 0:\n",
    "            print(\"\\n--- INFERENCE CHECK ---\")\n",
    "            with torch.no_grad():\n",
    "                inp_seq = inputs[0].tolist()\n",
    "                tar_seq = targets[0].tolist()\n",
    "                pred_seq = logits[0].argmax(dim=-1).tolist()\n",
    "                tok = ByteTokenizer()\n",
    "                print(f\"INPUT:  {tok.decode(inp_seq)[:80]}...\")\n",
    "                print(f\"TARGET: {tok.decode(tar_seq)[:80]}...\")\n",
    "                print(f\"MODEL:  {tok.decode(pred_seq)[:80]}...\")\n",
    "            print(\"-----------------------\\n\")\n",
    "            \n",
    "            # Save Checkpoint\n",
    "            ckpt_path = os.path.join(CKPT_DIR, f\"checkpoint_{step}.pt\")\n",
    "            torch.save({\n",
    "                'step': step,\n",
    "                'model_state': model.state_dict(),\n",
    "                'optimizer_state': optimizer.state_dict(),\n",
    "                'config': config, # Good practice to save config\n",
    "            }, ckpt_path)\n",
    "            print(f\"Saved checkpoint to {ckpt_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
